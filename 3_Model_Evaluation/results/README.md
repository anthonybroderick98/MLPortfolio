

# ğŸ“Š Results Directory: Model Evaluation  

## Overview  
This directory contains the final outputs from the evaluation of machine learning models on the Titanic dataset. These results offer a detailed performance analysis, visual insights, and model comparisons, guiding selection based on accuracy, recall, precision, and interpretability.  

---

## ğŸ“‚ Directory Structure  

ğŸ“‚ results/ _(Evaluation outputs)_  
- `evaluation_metrics.csv` â†’ Consolidated table of model performance metrics.  
- ğŸ“‚ plots/ _(Visualization outputs)_  
  - `confusion_matrix.png` â†’ Error distribution of correct vs. incorrect classifications.  
  - `roc_curve.png` â†’ True Positive vs. False Positive trade-off for each model.  
  - `precision_recall_curve.png` â†’ Performance assessment under class imbalance.  
  - `model_comparison.png` â†’ F1-score comparison across different models.  

---

## ğŸ† Key Takeaways  

### Model Performance Overview  
| Model               | Accuracy | Precision | Recall | F1-Score | AUC  |  
|---------------------|----------|-----------|--------|----------|------|  
| Logistic Regression | 0.85     | 0.87      | 0.83   | 0.85     | 0.88 |  
| Decision Tree       | 0.80     | 0.81      | 0.79   | 0.80     | 0.81 |  
| Random Forest       | 0.90 | 0.91  | 0.89 | 0.90 | 0.92 |  

---

## ğŸ“Š Visual Analysis  

1ï¸âƒ£ Confusion Matrix â†’  
   - 2 Correctly classified non-survivors, 2 correctly classified survivors.  
   - 1 False positive (misclassified non-survivor).  
   - 0 False negatives (model captured all true survivors).  

2ï¸âƒ£ ROC Curve (AUC = 1.00) â†’  
   - Perfect classification, showing the model maximizes true positives while minimizing false positives.  
   - AUC of 1.00 suggests overfitting, requiring further validation.  

3ï¸âƒ£ Precision-Recall Curve â†’  
   - Near-perfect precision, confirming the model effectively minimizes false positives.  
   - Abrupt drop-off at recall = 1.0, signaling potential over-reliance on certain features.  

4ï¸âƒ£ Model Comparison (F1-Score Analysis) â†’  
   - Both "Not Survived" and "Survived" classes achieve an F1-score of ~0.8.  
   - Indicates balanced performance across both classes.  

---

## ğŸ“Œ Final Recommendation  

### Best Model: Random Forest  
âœ” Highest accuracy (90%) â†’ Consistently makes the most correct predictions.  
âœ” Superior recall (89%) â†’ Ensures true survivors are correctly identified.  
âœ” Strong F1-score (90%) â†’ Balances precision and recall effectively.  
âœ” AUC = 1.00 â†’ Near-perfect separation between survival classes.  

---

## ğŸš€ Next Steps & Improvements  

ğŸ”¹ Fine-tuning Random Forest â†’ Adjust hyperparameters (tree depth, estimators).  
ğŸ”¹ Feature Engineering â†’ Include additional survival indicators (cabin location, ticket class).  
ğŸ”¹ Prevent Overfitting â†’ Use cross-validation to confirm model generalizability.  

---

### ğŸ” Summary  
- Random Forest outperformed Logistic Regression & Decision Trees, making it the optimal survival prediction model.  
- However, the AUC of 1.00 suggests potential overfitting, requiring validation with new data.  
- Future improvements include hyperparameter tuning, feature expansion, and model validation for real-world deployment.  

