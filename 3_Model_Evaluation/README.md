# Model Evaluation: Titanic Survival Prediction  

## Overview  
This project provides a structured evaluation of Supervised Learning models used to predict survival outcomes on the Titanic dataset. The objective is to assess model performance, interpret key insights, and determine the most effective model based on accuracy, interpretability, and real-world applicability.  

By analyzing Logistic Regression, Decision Trees, and Random Forest classifiers, we compare their performance across key metrics and visualize their strengths and weaknesses. The results guide model selection and highlight areas for improvement in predictive modeling.  

---

## ğŸ“‚ Code Directory Structure  

ğŸ“‚ code/ _(Core scripts for model evaluation & visualization)_  
â”œâ”€â”€ `evaluate_model.py` â†’ Runs full model evaluation and computes key metrics.  
â”œâ”€â”€ `fix_predictions.py` â†’ Cleans and formats the dataset for consistent evaluation.  
â”œâ”€â”€ `utils.py` â†’ Helper functions for metrics calculations and visualization.  
â”œâ”€â”€ `requirements.txt` â†’ Lists dependencies required for execution.  
â””â”€â”€ `README.md` _(This file â€“ Documentation for the scripts in `code/`)_  

ğŸ“‚ data/ _(Dataset storage)_  
â”œâ”€â”€ `predictions_data.csv` â†’ Model predictions for evaluation.  

ğŸ“‚ results/ _(Model evaluation outputs)_  
â”œâ”€â”€ `evaluation_metrics.csv` â†’ Consolidated table of model performance.  
â””â”€â”€ ğŸ“‚ plots/ _(Visualizations)_  
    â”œâ”€â”€ `confusion_matrix.png` â†’ Classification error heatmap.  
    â”œâ”€â”€ `roc_curve.png` â†’ ROC-AUC analysis.  
    â”œâ”€â”€ `precision_recall_curve.png` â†’ Precision vs. recall evaluation.  
    â”œâ”€â”€ `model_comparison.png` â†’ Comparative F1-score performance across models.  

---

## ğŸ”¹ Model Evaluation Breakdown  

### 1ï¸âƒ£ Model Performance Analysis  
âœ” Measure classification accuracy, precision, recall, and F1-score across models.  
âœ” Identify false positives and false negatives using Confusion Matrices.  
âœ” Assess decision boundaries through ROC and Precision-Recall Curves.  

### 2ï¸âƒ£ Visualizing Model Performance  
âœ” Generate Confusion Matrices, ROC Curves, Precision-Recall Curves for in-depth evaluation.  
âœ” Compare F1-scores across models to determine overall performance.  

### 3ï¸âƒ£ Key Takeaways  
âœ” Logistic Regression provides explainability but may struggle with complex decision boundaries.  
âœ” Decision Trees capture interactions well but are prone to overfitting.  
âœ” Random Forests offer strong predictive power with better generalization.  
âœ” Model selection is driven by trade-offs between accuracy, interpretability, and robustness.  

---

## ğŸ”¹ How to Run the Evaluation  

### 1ï¸âƒ£ Install Dependencies  
Ensure all necessary Python libraries are installed:  
```bash
pip install -r code/requirements.txt
2ï¸âƒ£ Run the Evaluation Script
Execute the model evaluation pipeline:

bash
Copy
Edit
python code/evaluate_model.py
3ï¸âƒ£ Review the Outputs
âœ” Performance metrics are saved in results/evaluation_metrics.csv.
âœ” Visualizations are generated in the results/plots/ directory.


ğŸ“© Questions & Contributions
ğŸ‘¤ Anthony Broderick
ğŸ“© For feedback, improvements, or suggestions, feel free to reach out.