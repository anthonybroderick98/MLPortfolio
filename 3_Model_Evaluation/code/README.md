
# Code Directory: Model Evaluation  

## Overview  
This directory contains all core scripts used for evaluating machine learning models and generating performance visualizations. The scripts automate **data preprocessing, model evaluation, and results visualization**, ensuring a structured and efficient analysis pipeline.  

---  

## ğŸ“‚ Directory Structure  

ğŸ“‚ **code/** _(Main directory for model evaluation)_  
- `evaluate_model.py` â†’ Runs the full model evaluation pipeline, computing metrics and generating visualizations.  
- `fix_predictions.py` â†’ Cleans and formats the predictions dataset before evaluation.  
- `utils.py` â†’ Helper functions for computing evaluation metrics and plotting results.  
- `requirements.txt` â†’ Lists required Python dependencies for running the scripts.  
- `README.md` _(This file â€“ Documentation for the scripts in `code/`)_  

---  

## ğŸ› ï¸ How It Works  

### 1ï¸âƒ£ **Fixing Predictions (Preprocessing)**  
âœ” Ensures the predictions dataset is correctly formatted.  
âœ” Removes inconsistencies and missing values.  
âœ” Standardizes column names for evaluation consistency.  

### 2ï¸âƒ£ **Evaluating Model Performance**  
âœ” Computes key classification metrics:  
   - **Accuracy, Precision, Recall, F1-Score**  
   - **Confusion Matrix for error analysis**  
   - **ROC Curve for model sensitivity vs. specificity**  
   - **Precision-Recall Curve for imbalanced data insights**  

### 3ï¸âƒ£ **Generating Model Insights & Visualizations**  
âœ” Saves performance metrics to a CSV file.  
âœ” Generates **confusion matrix, ROC curve, precision-recall curve**.  
âœ” Provides a **visual comparison of models** based on F1-scores.  

---  

## ğŸš€ Running the Evaluation Pipeline  

### 1ï¸âƒ£ Install Dependencies  
Ensure all required Python libraries are installed:  
```bash
pip install -r requirements.txt
```  

### 2ï¸âƒ£ Execute Model Evaluation  
Run the pipeline to process predictions, evaluate models, and generate plots:  
```bash
python evaluate_model.py
```  

### 3ï¸âƒ£ Review Outputs  
- Performance metrics saved in **`results/evaluation_metrics.csv`**  
- Visualizations generated in **`results/plots/`**  

---  

## ğŸ”¹ Next Steps & Potential Improvements  
âœ” Expand model comparison â†’ Include **additional models (e.g., SVM, Gradient Boosting)**.  
âœ” Automate **hyperparameter tuning** for optimal performance.  
âœ” Extend **feature engineering techniques** to improve model accuracy.  

---  

## ğŸ“© Questions & Collaboration  
ğŸ‘¤ **Anthony Broderick**  
ğŸ“© Feel free to reach out for contributions, improvements, or inquiries.  
```