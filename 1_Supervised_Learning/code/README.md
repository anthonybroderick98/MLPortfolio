# Machine Learning Pipeline â€“ Titanic Survival Prediction  

## Overview  
This directory contains the core machine learning scripts used to build and evaluate predictive models for Titanic passenger survival.  

The workflow follows a structured data science pipeline:  
âœ” Loading & exploring the dataset  
âœ” Preprocessing & feature engineering  
âœ” Splitting into training and testing sets  
âœ” Training classification models  
âœ” Evaluating performance with metrics & visualizations  

Each script in this directory plays a specific role in the project.

---

## ğŸ“‚ Code Directory Structure  

ğŸ“‚ code/ _(Main directory for model training & evaluation)_  
â”œâ”€â”€ `main.py` â†’ Executes the full pipeline: loads data, trains models, and evaluates performance.  
â”œâ”€â”€ `classification.py` â†’ Trains multiple classification models and compares their performance.  
â”œâ”€â”€ `utils.py` â†’ Helper functions for data preprocessing and feature engineering.  
â”œâ”€â”€ `requirements.txt` â†’ List of required Python libraries for running the scripts.  
â””â”€â”€ `README.md` _(This file â€“ Documentation for the scripts in `code/`)_  

---

## ğŸ”¹ Script Breakdown  

### 1ï¸âƒ£ `main.py` â€“ Full ML Pipeline Execution  
This script serves as the entry point for the project. It:  
âœ” Loads and preprocesses the Titanic dataset.  
âœ” Splits data into 80% training / 20% testing.  
âœ” Trains a Logistic Regression model.  
âœ” Evaluates performance using accuracy, confusion matrices, and classification reports.  

#### How to Run:  
```bash
python main.py
```

---

### 2ï¸âƒ£ `classification.py` â€“ Training & Comparing Models  
This script expands on `main.py` by:  
âœ” Training multiple models:  
   - Logistic Regression  
   - Decision Tree Classifier  
   - Random Forest Classifier  
âœ” Evaluating models on accuracy, precision, recall, F1-score, and ROC-AUC.  
âœ” Saving confusion matrices & feature importance visualizations for model interpretability.  

#### How to Run:  
```bash
python classification.py
```

---

### 3ï¸âƒ£ `utils.py` â€“ Data Preprocessing & Feature Engineering  
This script contains reusable functions for:  
âœ” Handling missing values using median and mode imputation.  
âœ” Encoding categorical variables (`Sex`, `Embarked`).  
âœ” Creating new features like `FamilySize` to improve model accuracy.  
âœ” Splitting datasets into training and testing sets.  

This module is imported in `main.py` and `classification.py`, ensuring a clean and modular codebase.

---

## ğŸ“Œ Requirements & Setup  

### ğŸ”¹ Python Version  
- Python 3.x  

### ğŸ”¹ Required Libraries  
Install dependencies before running the scripts:  
```bash
pip install -r requirements.txt
```  

---

## ğŸ“Œ Next Steps & Further Improvements  
âœ” Expand model comparison â†’ Include SVM or Gradient Boosting.  
âœ” Feature Engineering â†’ Explore additional transformations.  
âœ” Hyperparameter Tuning â†’ Optimize models for better accuracy.  
âœ” Automated Reporting â†’ Save metrics & visualizations for easy review.  

---

## ğŸ“Œ Where to Find Model Results?  
The trained model outputs, predictions, and visualizations are stored in:  
ğŸ“‚ `results/` â†’ _(See `results/README.md` for insights and interpretations)_  

---

## ğŸ“© Questions & Collaboration  
ğŸ‘¤ Anthony Broderick  
ğŸ“© For contributions, questions, or improvements, feel free to reach out.  
